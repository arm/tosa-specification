//
// This confidential and proprietary software may be used only as
// authorised by a licensing agreement from ARM Limited
// (C) COPYRIGHT 2020-2024 ARM Limited
// ALL RIGHTS RESERVED
// The entire notice above must be reproduced on all authorised
// copies and copies may only be made to the extent permitted
// by a licensing agreement from ARM Limited.

=== Reduction Operators

==== REDUCE_ALL

Reduce a tensor along the given axis with a logical AND operation

*Precision Requirements*

Results must be exact.

include::{generated}/operators/REDUCE_ALL.adoc[]

[source,c]
----
include::{pseudocode}/operators/REDUCE_ALL.tosac[lines=10..-1]
----

==== REDUCE_ANY

Reduce a tensor along the given axis with a logical OR operation

*Precision Requirements*

Results must be exact.

include::{generated}/operators/REDUCE_ANY.adoc[]

[source,c]
----
include::{pseudocode}/operators/REDUCE_ANY.tosac[lines=10..-1]
----

==== REDUCE_MAX

Reduce a tensor along the given axis with a maximum operation

*Precision Requirements*

Integer results must be exact.

NaN propagation mode only affects floating-point types.
It indicates either propagating or ignoring NaN.

The following rules apply to floating-point inputs:

* Comparison rules:
** The sign of a zero is ignored.
** Infinities of the same sign compare as equal.
** In the NaN propagating mode, if any input value along the reduction axis is a NaN, the result is NaN.
** In the NaN ignoring mode, if all input values along the reduction axis are NaN, the result is NaN.
Otherwise the result is the maximum non-NaN value.
* Subnormal bf16_t, fp16_t, and fp32_t input values may be flushed to zero before calculation.
* If a floating-point result is zero, then the result must be either +0.0 or -0.0 but either sign is permitted.
* If the result is a subnormal value for bf16_t, fp16_t, or fp32_t, the result may be a zero of either sign.
* Otherwise floating-point results must be exact.

include::{generated}/operators/REDUCE_MAX.adoc[]

[source,c]
----
include::{pseudocode}/operators/REDUCE_MAX.tosac[lines=10..-1]
----

==== REDUCE_MIN

Reduce a tensor along the given axis with a minimum operation

*Precision Requirements*

Integer results must be exact.

NaN propagation mode only affects floating-point types.
It indicates either propagating or ignoring NaN.

The following rules apply to floating-point inputs:

* Comparison rules:
** The sign of a zero is ignored.
** Infinities of the same sign compare as equal.
** In the NaN propagating mode, if any input value along the reduction axis is a NaN, the result is NaN.
** In the NaN ignoring mode, if all input values along the reduction axis are NaN, the result is NaN.
Otherwise the result is the minimum non-NaN value.
* Subnormal bf16_t, fp16_t, and fp32_t input values may be flushed to zero before calculation.
* If a floating-point result is zero, then the result must be either +0.0 or -0.0 but either sign is permitted.
* If the result is a subnormal value for bf16_t, fp16_t, or fp32_t, the result may be a zero of either sign.
* Otherwise floating-point results must be exact.

include::{generated}/operators/REDUCE_MIN.adoc[]

[source,c]
----
include::{pseudocode}/operators/REDUCE_MIN.tosac[lines=10..-1]
----

==== REDUCE_PRODUCT

Reduce a tensor along the given axis by computing the product of the axis.

*Precision Requirements*

* Subnormal bf16_t, fp16_t, and fp32_t input values may be flushed to zero before calculation.
* If the input is a NaN, the output must be a NaN.
** Otherwise the following may be used to validate the result:
** Let n be number of elements in the product, out_imp the implementation result, and out_ref the result of the fp64_t reference implementation.
** Let `err_bnd = max(abs(out_ref), normal_min<in_out_t>()) * (pow(1 + pow(2, -normal_frac<in_out_t>() - 1), n) - 1)`.
** Then `tosa_reference_check_fp_bnd<in_out_t>(out_imp, out_ref, err_bnd)` must be true.

include::{generated}/operators/REDUCE_PRODUCT.adoc[]

[source,c]
----
include::{pseudocode}/operators/REDUCE_PRODUCT.tosac[lines=10..-1]
----

==== REDUCE_SUM

Reduce a tensor along the given axis by computing the sum of the axis.

*Precision Requirements*

Integer results must be exact.

Floating-point outputs can be expressed as a dot product of an input vector with a vector of ones.
This dot product must meet the <<Dot product accuracy requirements>>.

include::{generated}/operators/REDUCE_SUM.adoc[]

[source,c]
----
include::{pseudocode}/operators/REDUCE_SUM.tosac[lines=10..-1]
----
