//
// This confidential and proprietary software may be used only as
// authorised by a licensing agreement from ARM Limited
// (C) COPYRIGHT 2020-2023 ARM Limited
// ALL RIGHTS RESERVED
// The entire notice above must be reproduced on all authorised
// copies and copies may only be made to the extent permitted
// by a licensing agreement from ARM Limited.

=== Elementwise Binary Operators

==== ADD

Elementwise addition of input1 and input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/ADD.adoc[]

[source,c++]
----
if (in_out_t == shape_t) {
    ERROR_IF(rank(shape) != 0 || rank(shape1) != 0 || rank(shape2) != 0);
    shape_t value1 = tensor_read<shape_t>(input1, [], []);
    shape_t value2 = tensor_read<shape_t>(input2, [], []);
    shape_t result = apply_add_s<shape_t>(value1, value2);
    tensor_write<shape_t>(output, [], [], result);
} else {
    ERROR_IF(shape != broadcast_shape(shape1, shape2));
    for_each(index in shape) {
        dim_t index1 = apply_broadcast(shape, shape1, index);
        dim_t index2 = apply_broadcast(shape, shape2, index);
        in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
        in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
        in_out_t result = apply_add_s<in_out_t>(value1, value2);
        tensor_write<in_out_t>(output, shape, index, result);
    }
}
----

==== ARITHMETIC_RIGHT_SHIFT

Elementwise arithmetic right shift of input1 by the amount specified in input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/ARITHMETIC_RIGHT_SHIFT.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);

    // Ensure that shift amount is appropriate for the data type
    REQUIRE((in_out_t == i32_t && 0 <= value2 && value2 <= 31) ||
            (in_out_t == i16_t && 0 <= value2 && value2 <= 15) ||
            (in_out_t == i8_t && 0 <= value2 && value2 <= 7));

    in_out_t result = apply_arith_rshift<in_out_t>(value1, value2);
    if (round == true && static_cast<int32_t>(value2) > 0 &&
        (apply_arith_rshift<in_out_t>(value1, apply_sub_s<in_out_t>(value2, 1)) & 1 != 0) {
        result = result + 1;
    }
    result = apply_clip_s<in_out_t>(result, minimum_s<in_out_t>, maximum_s<in_out_t>);
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== BITWISE_AND

Elementwise bitwise AND of input1 and input2.
Axis of size 1 will be broadcast as necessary. Rank of input tensors must match.

include::{generated}/operators/BITWISE_AND.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = value1 & value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== BITWISE_OR

Elementwise bitwise OR of input1 and input2.
Axis of size 1 will be broadcast as necessary. Rank of input tensors must match.

include::{generated}/operators/BITWISE_OR.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = value1 | value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== BITWISE_XOR

Elementwise bitwise XOR of input1 and input2.
Axis of size 1 will be broadcast as necessary. Rank of input tensors must match.

include::{generated}/operators/BITWISE_XOR.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = value1 ^ value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== INTDIV

Elementwise integer divide of input1 by input2.
The result of the divide is truncated towards zero.
Expected use is for operations on non-scaled integers.
Floating point divide should use RECIPROCAL and MUL.
Quantized integer divide should use TABLE (for 1/x) and MUL.

include::{generated}/operators/INTDIV.adoc[]

[source,c++]
----
if (in_out_t == shape_t) {
    ERROR_IF(rank(shape) != 0 || rank(shape1) != 0 || rank(shape2) != 0);
    shape_t value1 = tensor_read<shape_t>(input1, [], []);
    shape_t value2 = tensor_read<shape_t>(input2, [], []);
    REQUIRE(value2 != 0);
    shape_t result = value1 / value2;
    tensor_write<shape_t>(output, [], [], result);
} else {
    ERROR_IF(shape != broadcast_shape(shape1, shape2));
    for_each(index in shape) {
        dim_t index1 = apply_broadcast(shape, shape1, index);
        dim_t index2 = apply_broadcast(shape, shape2, index);
        in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
        in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
        REQUIRE(value2 != 0);
        // This catches the case where we divide minimum<in_out_t> by -1
        // which is not representable in two's complement
        REQUIRE(static_cast<int64_t>(value1) / static_cast<int64_t>(value2) <= maximum_s<in_out_t>);
        in_out_t result = apply_intdiv_s<in_out_t>(value1, value2);
        tensor_write<in_out_t>(output, shape, index, result);
    }
}
----

==== LOGICAL_AND

Elementwise logical AND of input1 and input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/LOGICAL_AND.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = value1 && value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== LOGICAL_LEFT_SHIFT

Elementwise logical left shift of input1 by the amount specified in input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/LOGICAL_LEFT_SHIFT.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    REQUIRE(0 <= value2 && value2 <= 31);
    in_out_t result = value1 << value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== LOGICAL_RIGHT_SHIFT

Elementwise logical right shift of input1 by the amount specified in input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/LOGICAL_RIGHT_SHIFT.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    REQUIRE(0 <= static_cast<int32_t>(value2) && static_cast<int32_t>(value2) <= 31);
    // Logical shifts happen as unsigned types internally
    in_out_t result = apply_logical_rshift<in_out_t>(value1, value2);
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== LOGICAL_OR

Elementwise logical OR of input1 and input2.
Axis of size 1 will be broadcast as necessary. Rank of input tensors must match.

include::{generated}/operators/LOGICAL_OR.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = value1 || value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== LOGICAL_XOR

Elementwise logical XOR of input1 and input2.
Axis of size 1 will be broadcast as necessary. Rank of input tensors must match.

include::{generated}/operators/LOGICAL_XOR.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = value1 != value2;
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== MAXIMUM

Elementwise max of input1 and input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/MAXIMUM.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = apply_max_s<in_out_t>(value1, value2);
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== MINIMUM

Elementwise minimum of input1 and input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/MINIMUM.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = apply_min_s(value1, value2);
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== MUL

Elementwise multiplication (Hadamard product) of input1 and input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/MUL.adoc[]

[source,c++]
----
if (in_out_t == shape_t) {
    ERROR_IF(rank(shape) != 0 || rank(shape1) != 0 || rank(shape2) != 0);
    shape_t value1 = tensor_read<shape_t>(input1, [], []);
    shape_t value2 = tensor_read<shape_t>(input2, [], []);
    shape_t result = value1 * value2;
    tensor_write<shape_t>(output, [], [], result);
} else {
    REQUIRE(0 <= shift && shift <= 63);
    REQUIRE(in_t == int32_t || shift == 0);
    ERROR_IF(shape != broadcast_shape(shape1, shape2));
    for_each(index in shape) {
        dim_t index1 = apply_broadcast(shape, shape1, index);
        dim_t index2 = apply_broadcast(shape, shape2, index);
        in_t value1 = tensor_read<in_t>(input1, shape1, index1);
        in_t value2 = tensor_read<in_t>(input2, shape2, index2);
        out_t result;
        if (in_t == i32_t && shift > 0) {
            int64_t product = sign_extend<int64_t>(value1) * sign_extend<int64_t>(value2);
            int64_t round   = static_cast<int64_t>(1) << (shift - 1);
            product = (product + round) >> shift;
            REQUIRE(product >= minimum_s<i32_t> && product <= maximum_s<i32_t>)
            result = product;
        } else {
            result = apply_mul_s(value1, value2);  // low 32-bits of result for i32_t
        }
        tensor_write<out_t>(output, shape, index, result);
    }
}
----

==== POW

Elementwise input1 value raised to the power of input2.
Axis of size 1 will be broadcast, as necessary. Rank of input tensors must match.

include::{generated}/operators/POW.adoc[]

[source,c++]
----
ERROR_IF(shape != broadcast_shape(shape1, shape2));
for_each(index in shape) {
    dim_t index1 = apply_broadcast(shape, shape1, index);
    dim_t index2 = apply_broadcast(shape, shape2, index);
    in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
    in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
    in_out_t result = apply_pow<in_out_t>(value1, value2);
    tensor_write<in_out_t>(output, shape, index, result);
}
----

==== SUB

Elementwise subtraction of input1 and input2.
Axis of size 1 will be broadcast as necessary. Rank of input tensors must match.

include::{generated}/operators/SUB.adoc[]

[source,c++]
----
if (in_out_t == shape_t) {
    ERROR_IF(rank(shape) != 0 || rank(shape1) != 0 || rank(shape2) != 0);
    shape_t value1 = tensor_read<shape_t>(input1, [], []);
    shape_t value2 = tensor_read<shape_t>(input2, [], []);
    shape_t result = apply_sub<shape_t>(value1, value2);
    tensor_write<shape_t>(output, [], [], result);
} else {
    ERROR_IF(shape != broadcast_shape(shape1, shape2));
    for_each(index in shape) {
        dim_t index1 = apply_broadcast(shape, shape1, index);
        dim_t index2 = apply_broadcast(shape, shape2, index);
        in_out_t value1 = tensor_read<in_out_t>(input1, shape1, index1);
        in_out_t value2 = tensor_read<in_out_t>(input2, shape2, index2);
        in_out_t result = apply_sub_s<in_out_t>(value1, value2);
        tensor_write<in_out_t>(output, shape, index, result);
    }
}
----

====   TABLE

Table lookup operation.
For int8_t TABLE operation, perform a 256 entry table lookup returning an int8_t value.
For int16_t tables, the int16_t input is treated as a fixed-point 9.7 value.
The most significant 9 bits are used to index into the table.
The fractional 7 bits are used to interpolate based on table[index] and table[index+1].
For int16_t inputs, the TABLE operator returns a 16.7 interpolated value in an int32_t.
This value can then be input to the RESCALE operator to scale to the required output data type.
Note that int16_t table has 513 values to handle table[index+1] when index=511.

An int16_t to int16_t table lookup can be constructed in TOSA as follows:

* Use the TABLE operator to produce a fixed point 16.7 interpolated result
* Use RESCALE (in_t=int32_t, out_t=int16_t, scale=1<<14, shift=21) to scale the output to int16_t range (or alternate scale as required)

include::{generated}/operators/TABLE.adoc[]

[source,c++]
----
REQUIRE(length(table) == TABLE_SIZE);
for_each(index in shape) {
    in_t value = tensor_read<in_t>(input, shape, index);
    out_t result;
    if (in_t == i8_t) {
        // value is a signed int, convert to a 0 based index
        result = table[static_cast<int16_t>(value) + 128];
    } else {
        result = apply_lookup_s(static_cast<int16_t>(table), static_cast<int16_t>(value));
    }
    tensor_write<out_t>(output, shape, index, result);
}
----
